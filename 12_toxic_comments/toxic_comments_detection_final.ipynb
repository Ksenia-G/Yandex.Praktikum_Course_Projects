{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FiITYAoBwmJ"
   },
   "source": [
    "# Описание проекта\n",
    "\n",
    "**Заказчик:** Интернет-магазин «Викишоп».\n",
    "\n",
    "**Цель:** Создание инструмента для осуществления поиска токсичных комментариев для последующего направления их на модерацию.\n",
    "\n",
    "**Подцель:** создать модель машинного обучения (далее - МО) и обучить МО классифицировать комментарии на позитивные и негативные. Значение метрики качества F1 не должно быть ниже 0.75.\n",
    "\n",
    "**Данные:** набор данных с разметкой о токсичности правок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Подготовка ноутбука к работе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TWBeiF_BAsb"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HiQaqDXARgLm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rj3ryDbEIHNv",
    "outputId": "da972c42-9179-4f6b-9f6f-d80b57ef0ada"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kseniagabrusevich/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kseniagabrusevich/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QzC2bQ0lBfd2"
   },
   "outputs": [],
   "source": [
    "def display_main_info(df):\n",
    "    print('________________Head________________')\n",
    "    display(df.head(15))\n",
    "    print('________________Data Types Info________________')\n",
    "    display(df.info())\n",
    "    print('________________Shape________________')\n",
    "    display(df.shape)\n",
    "    print('________________Duplicates________________')\n",
    "    display(df.duplicated().sum())\n",
    "    print('________________Proportion of Missing Data________________')\n",
    "    display(df.isna().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Загрузка и анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OFn1uJnfBkq5"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('https://toxic_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "twVZ0ylECRtp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________Head________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bbq \\n\\nbe a man and lets discuss it-maybe ove...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Before you start throwing accusations and warn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Oh, and the girl above started her arguments w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  toxic\n",
       "0   Explanation\\nWhy the edits made under my usern...      0\n",
       "1   D'aww! He matches this background colour I'm s...      0\n",
       "2   Hey man, I'm really not trying to edit war. It...      0\n",
       "3   \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4   You, sir, are my hero. Any chance you remember...      0\n",
       "5   \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7   Your vandalism to the Matt Shirvington article...      0\n",
       "8   Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9   alignment on this subject and which are contra...      0\n",
       "10  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...      0\n",
       "11  bbq \\n\\nbe a man and lets discuss it-maybe ove...      0\n",
       "12  Hey... what is it..\\n@ | talk .\\nWhat is it......      1\n",
       "13  Before you start throwing accusations and warn...      0\n",
       "14  Oh, and the girl above started her arguments w...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________Data Types Info________________\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________Shape________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(159292, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________Duplicates________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________Proportion of Missing Data________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text     0.0\n",
       "toxic    0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_main_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8NJpc2KzCULu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдения:**\n",
    "\n",
    "- Число наблюдений 159292.\n",
    "- Пропусков и дубликатов (полных) не обнаружено.\n",
    "- Данные размечены: таргет `toxic`: 1 - комментарий токсичный, 0 - комментарий не токсичный. При этом, есть большой дисбаланс классов, токсичные комментарии составляют только 10% датасета. Это необходимо учесть при формировании выборок и в гиперпараметрах модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTp5CttIAp-v"
   },
   "source": [
    "Подготовка данных будет включать в себя очистку текстов от специальных символов, токенизацию и лемматизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_5SIQRivAMVJ"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "q-9c_rVfCBtz"
   },
   "outputs": [],
   "source": [
    "df['text_preprocessed'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9S_Rzb-WEIzT"
   },
   "outputs": [],
   "source": [
    "def tokenize_lemmatize(text):\n",
    "\n",
    "  # Tokenization\n",
    "  tokenizer = WhitespaceTokenizer()\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "  # Lemmatization\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmed_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "  return ' '.join(lemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LNjeb66jIfov"
   },
   "outputs": [],
   "source": [
    "df['lemmed_text'] = df['text_preprocessed'].apply(tokenize_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iFPID1c-JUaD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>lemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128856</th>\n",
       "      <td>Here's why it has to be analytic not just at  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>heres why it has to be analytic not just at  i...</td>\n",
       "      <td>here analytic imagine analytic within informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44162</th>\n",
       "      <td>(unblock/ why is my computer blocked i coe hom...</td>\n",
       "      <td>0</td>\n",
       "      <td>unblock why is my computer blocked i coe home ...</td>\n",
       "      <td>unblock computer blocked coe home hard day wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113775</th>\n",
       "      <td>Further please note that the vitiated nature o...</td>\n",
       "      <td>0</td>\n",
       "      <td>further please note that the vitiated nature o...</td>\n",
       "      <td>please note vitiated nature called documentary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15070</th>\n",
       "      <td>Hi\\n\\nLook, I am still here, and always will b...</td>\n",
       "      <td>0</td>\n",
       "      <td>hi  look i am still here and always will be en...</td>\n",
       "      <td>hi look still always enter fruitful dialogue l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48784</th>\n",
       "      <td>Expanded a bit. Will see if I can find out a b...</td>\n",
       "      <td>0</td>\n",
       "      <td>expanded a bit will see if i can find out a bi...</td>\n",
       "      <td>expanded bit see find bit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "128856  Here's why it has to be analytic not just at  ...      0   \n",
       "44162   (unblock/ why is my computer blocked i coe hom...      0   \n",
       "113775  Further please note that the vitiated nature o...      0   \n",
       "15070   Hi\\n\\nLook, I am still here, and always will b...      0   \n",
       "48784   Expanded a bit. Will see if I can find out a b...      0   \n",
       "\n",
       "                                        text_preprocessed  \\\n",
       "128856  heres why it has to be analytic not just at  i...   \n",
       "44162   unblock why is my computer blocked i coe home ...   \n",
       "113775  further please note that the vitiated nature o...   \n",
       "15070   hi  look i am still here and always will be en...   \n",
       "48784   expanded a bit will see if i can find out a bi...   \n",
       "\n",
       "                                              lemmed_text  \n",
       "128856  here analytic imagine analytic within informat...  \n",
       "44162   unblock computer blocked coe home hard day wor...  \n",
       "113775  please note vitiated nature called documentary...  \n",
       "15070   hi look still always enter fruitful dialogue l...  \n",
       "48784                           expanded bit see find bit  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "n8uK5sXOODIb"
   },
   "outputs": [],
   "source": [
    "df_final = df.drop(columns=['text', 'text_preprocessed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Создание и обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделирование будет осуществляться с помощью векторизации текстов инструментом TfidfVectorizer.\n",
    "\n",
    "В качестве моделей будут тестироваться:\n",
    "\n",
    "- Logistic Regression Classifier - хорошая базовая модель. Быстрая, интерпретируемая, устойчива к переобучению при использовании регуляризации. Дополнительно, применим логарифмическое преобразование TF. \n",
    "\n",
    "- Passive Aggressive Classifier. Эффективна для больших текстовых данных и задач с дисбалансом классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o31NxL7w0s0u"
   },
   "outputs": [],
   "source": [
    "features = df_final['lemmed_text']\n",
    "target = df_final['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cM-YC-MEN4XQ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    target,\n",
    "    test_size=0.2,\n",
    "    stratify=target, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ukjCWQifGbn6"
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, vectorizer, hyperparametres, X, y):\n",
    "\n",
    "  if len(X) == 0 or len(y) == 0:\n",
    "      raise ValueError(\"X и y не должны быть пустыми.\")\n",
    "\n",
    "  model_pipe = Pipeline(steps=[\n",
    "      ('vectorizer', vectorizer),\n",
    "      ('classifier', classifier)\n",
    "  ])\n",
    "\n",
    "\n",
    "  model_search = RandomizedSearchCV(\n",
    "      estimator=model_pipe,\n",
    "      param_distributions=hyperparametres,\n",
    "      n_iter=15,\n",
    "      scoring='f1',\n",
    "      n_jobs=-1,\n",
    "      cv=5,\n",
    "      verbose=0,\n",
    "      random_state=42\n",
    "      )\n",
    "\n",
    "  model_search.fit(X, y)\n",
    "\n",
    "  print('\\nЛучшая метрика на кросс-валидации: ', model_search.best_score_)\n",
    "  print('\\nЛучшая конфигурация', model_search.best_params_)\n",
    "\n",
    "  result = model_search.best_estimator_\n",
    "\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecorizer \n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_uwUJ54HrAn",
    "outputId": "0703e334-e312-4572-e3c6-0c1d968bd9d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 12 is smaller than n_iter=15. Running 12 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Лучшая метрика на кросс-валидации:  0.7624446832776727\n",
      "\n",
      "Лучшая конфигурация {'vectorizer__max_features': 50000, 'classifier__penalty': 'l2', 'classifier__C': 10}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    random_state=42,\n",
    "    solver='liblinear',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "params_lr = {\n",
    "        'vectorizer__max_features': [10000, 50000],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__C': [0.1, 1.0, 10]\n",
    "}\n",
    "\n",
    "log_reg = train_model(\n",
    "    classifier=lr,\n",
    "    vectorizer=vectorizer,\n",
    "    hyperparametres=params_lr,\n",
    "    X=X_train,\n",
    "    y=y_train\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GjmXEnOo_RZ8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Лучшая метрика на кросс-валидации:  0.7598825233502954\n",
      "\n",
      "Лучшая конфигурация {'vectorizer__max_features': 50000, 'classifier__early_stopping': False, 'classifier__average': True, 'classifier__C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Passive Aggressive Classifier\n",
    "\n",
    "pac = PassiveAggressiveClassifier(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "params_pac = {\n",
    "    'vectorizer__max_features': [10000, 50000],\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10],  \n",
    "    'classifier__early_stopping': [True, False],  \n",
    "    'classifier__average': [True, False]  \n",
    "}\n",
    "\n",
    "pac_model = train_model(\n",
    "    classifier=pac,\n",
    "    vectorizer=vectorizer,\n",
    "    hyperparametres=params_pac,\n",
    "    X=X_train,\n",
    "    y=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия показала себя лучше. Проверим качество на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TeZ1pdXNHq5i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скор на тесте: 0.765496315561335\n"
     ]
    }
   ],
   "source": [
    "y_preds = log_reg.predict(X_test)\n",
    "f_1 = f1_score(y_test, y_preds)\n",
    "print('Скор на тесте:', f_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалось хорошо обучить модель при этом получив нужную метрику на тестовой выборке и избежать переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы по проекту:**\n",
    "\n",
    "- В рамках данного проекта был очищен и подготовлен датасет с набором твитов.\n",
    "- Проведена векторизация текстов и выбрана оптимальная модель в следующей кофигурации:\n",
    "\n",
    "    - `vectorizer__max_features`: 50000, \n",
    "    - `classifier__penalty`: 'l2', \n",
    "    - `classifier__C`: 10\n",
    "- Метрика на тествой выборке составила ~**0.77**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
